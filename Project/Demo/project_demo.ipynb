{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceType":"modelInstanceVersion","sourceId":642787,"databundleVersionId":14457275,"modelInstanceId":484690}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"!pip install protobuf==3.20.3\n!pip install transformers-interpret\n!CMAKE_ARGS=\"-DGGML_CUDA=on\" pip install llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu121","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T01:27:00.097970Z","iopub.execute_input":"2025-11-27T01:27:00.098240Z","iopub.status.idle":"2025-11-27T01:28:49.688181Z","shell.execute_reply.started":"2025-11-27T01:27:00.098220Z","shell.execute_reply":"2025-11-27T01:28:49.687181Z"}},"outputs":[{"name":"stdout","text":"Collecting protobuf==3.20.3\n  Downloading protobuf-3.20.3-py2.py3-none-any.whl.metadata (720 bytes)\nDownloading protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: protobuf\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 6.33.0\n    Uninstalling protobuf-6.33.0:\n      Successfully uninstalled protobuf-6.33.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\nopentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 3.20.3 which is incompatible.\nonnx 1.18.0 requires protobuf>=4.25.1, but you have protobuf 3.20.3 which is incompatible.\na2a-sdk 0.3.10 requires protobuf>=5.29.5, but you have protobuf 3.20.3 which is incompatible.\nray 2.51.1 requires click!=8.3.0,>=7.0, but you have click 8.3.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\ntensorflow-metadata 1.17.2 requires protobuf>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\nydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\ngrpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.3 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed protobuf-3.20.3\nCollecting transformers-interpret\n  Downloading transformers_interpret-0.10.0-py3-none-any.whl.metadata (45 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.9/45.9 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting captum>=0.3.1 (from transformers-interpret)\n  Downloading captum-0.8.0-py3-none-any.whl.metadata (26 kB)\nRequirement already satisfied: ipython<8.0.0,>=7.31.1 in /usr/local/lib/python3.11/dist-packages (from transformers-interpret) (7.34.0)\nRequirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from transformers-interpret) (4.53.3)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from captum>=0.3.1->transformers-interpret) (3.7.2)\nRequirement already satisfied: numpy<2.0 in /usr/local/lib/python3.11/dist-packages (from captum>=0.3.1->transformers-interpret) (1.26.4)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from captum>=0.3.1->transformers-interpret) (25.0)\nRequirement already satisfied: torch>=1.10 in /usr/local/lib/python3.11/dist-packages (from captum>=0.3.1->transformers-interpret) (2.6.0+cu124)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from captum>=0.3.1->transformers-interpret) (4.67.1)\nRequirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython<8.0.0,>=7.31.1->transformers-interpret) (75.2.0)\nRequirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from ipython<8.0.0,>=7.31.1->transformers-interpret) (0.19.2)\nRequirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython<8.0.0,>=7.31.1->transformers-interpret) (4.4.2)\nRequirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython<8.0.0,>=7.31.1->transformers-interpret) (0.7.5)\nRequirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.11/dist-packages (from ipython<8.0.0,>=7.31.1->transformers-interpret) (5.7.1)\nRequirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython<8.0.0,>=7.31.1->transformers-interpret) (3.0.51)\nRequirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython<8.0.0,>=7.31.1->transformers-interpret) (2.19.2)\nRequirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython<8.0.0,>=7.31.1->transformers-interpret) (0.2.0)\nRequirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython<8.0.0,>=7.31.1->transformers-interpret) (0.1.7)\nRequirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython<8.0.0,>=7.31.1->transformers-interpret) (4.9.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->transformers-interpret) (3.20.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->transformers-interpret) (0.36.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->transformers-interpret) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->transformers-interpret) (2025.11.3)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->transformers-interpret) (2.32.5)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->transformers-interpret) (0.21.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->transformers-interpret) (0.5.3)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers>=3.0.0->transformers-interpret) (2025.10.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers>=3.0.0->transformers-interpret) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers>=3.0.0->transformers-interpret) (1.2.0)\nRequirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython<8.0.0,>=7.31.1->transformers-interpret) (0.8.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<2.0->captum>=0.3.1->transformers-interpret) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<2.0->captum>=0.3.1->transformers-interpret) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<2.0->captum>=0.3.1->transformers-interpret) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<2.0->captum>=0.3.1->transformers-interpret) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<2.0->captum>=0.3.1->transformers-interpret) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<2.0->captum>=0.3.1->transformers-interpret) (2.4.1)\nRequirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython<8.0.0,>=7.31.1->transformers-interpret) (0.7.0)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython<8.0.0,>=7.31.1->transformers-interpret) (0.2.13)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum>=0.3.1->transformers-interpret) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum>=0.3.1->transformers-interpret) (3.1.6)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.10->captum>=0.3.1->transformers-interpret)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.10->captum>=0.3.1->transformers-interpret)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.10->captum>=0.3.1->transformers-interpret)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.10->captum>=0.3.1->transformers-interpret)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.10->captum>=0.3.1->transformers-interpret)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.10->captum>=0.3.1->transformers-interpret)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.10->captum>=0.3.1->transformers-interpret)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.10->captum>=0.3.1->transformers-interpret)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.10->captum>=0.3.1->transformers-interpret)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum>=0.3.1->transformers-interpret) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum>=0.3.1->transformers-interpret) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum>=0.3.1->transformers-interpret) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.10->captum>=0.3.1->transformers-interpret)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum>=0.3.1->transformers-interpret) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum>=0.3.1->transformers-interpret) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.10->captum>=0.3.1->transformers-interpret) (1.3.0)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum>=0.3.1->transformers-interpret) (1.3.2)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum>=0.3.1->transformers-interpret) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum>=0.3.1->transformers-interpret) (4.59.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum>=0.3.1->transformers-interpret) (1.4.8)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum>=0.3.1->transformers-interpret) (11.3.0)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum>=0.3.1->transformers-interpret) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum>=0.3.1->transformers-interpret) (2.9.0.post0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=3.0.0->transformers-interpret) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=3.0.0->transformers-interpret) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=3.0.0->transformers-interpret) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=3.0.0->transformers-interpret) (2025.10.5)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->captum>=0.3.1->transformers-interpret) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.10->captum>=0.3.1->transformers-interpret) (3.0.3)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.0->captum>=0.3.1->transformers-interpret) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.0->captum>=0.3.1->transformers-interpret) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.0->captum>=0.3.1->transformers-interpret) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<2.0->captum>=0.3.1->transformers-interpret) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<2.0->captum>=0.3.1->transformers-interpret) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<2.0->captum>=0.3.1->transformers-interpret) (2024.2.0)\nDownloading transformers_interpret-0.10.0-py3-none-any.whl (45 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading captum-0.8.0-py3-none-any.whl (1.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m98.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m92.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, captum, transformers-interpret\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed captum-0.8.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 transformers-interpret-0.10.0\nLooking in indexes: https://pypi.org/simple, https://abetlen.github.io/llama-cpp-python/whl/cu121\nCollecting llama-cpp-python\n  Downloading https://github.com/abetlen/llama-cpp-python/releases/download/v0.3.16-cu121/llama_cpp_python-0.3.16-cp311-cp311-linux_x86_64.whl (551.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m551.3/551.3 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (4.15.0)\nRequirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (1.26.4)\nCollecting diskcache>=5.6.1 (from llama-cpp-python)\n  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (3.1.6)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20.0->llama-cpp-python) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20.0->llama-cpp-python) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20.0->llama-cpp-python) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20.0->llama-cpp-python) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20.0->llama-cpp-python) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20.0->llama-cpp-python) (2.4.1)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.20.0->llama-cpp-python) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.20.0->llama-cpp-python) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.20.0->llama-cpp-python) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.20.0->llama-cpp-python) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.20.0->llama-cpp-python) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.20.0->llama-cpp-python) (2024.2.0)\nDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: diskcache, llama-cpp-python\nSuccessfully installed diskcache-5.6.3 llama-cpp-python-0.3.16\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import gradio as gr\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline, AutoConfig\nimport transformers\nimport shap\nfrom transformers_interpret import SequenceClassificationExplainer\nfrom llama_cpp import Llama\nfrom huggingface_hub import snapshot_download\nimport pandas as pd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T01:28:49.689841Z","iopub.execute_input":"2025-11-27T01:28:49.690144Z","iopub.status.idle":"2025-11-27T01:29:26.568058Z","shell.execute_reply.started":"2025-11-27T01:28:49.690118Z","shell.execute_reply":"2025-11-27T01:29:26.567361Z"}},"outputs":[{"name":"stderr","text":"2025-11-27 01:29:06.565618: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1764206946.749254      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1764206946.801925      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Model configurations\nMODEL_CONFIGS = {\n    \"BERT\": \"/kaggle/input/disorbert-finetuned-models/pytorch/default/1/finetuned-models/bert-base-cased-finetuned/checkpoint-5530\",\n    \"RoBERTa\": \"/kaggle/input/disorbert-finetuned-models/pytorch/default/1/finetuned-models/roberta-base-finetuned/checkpoint-6320\", \n    \"MentalBERT\": \"/kaggle/input/disorbert-finetuned-models/pytorch/default/1/finetuned-models/deberta-v3-base-finetuned/checkpoint-4345\",\n}\n\nLABEL_NAMES = [\"Normal\", \"Depressed\"] \ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T01:29:26.568758Z","iopub.execute_input":"2025-11-27T01:29:26.569346Z","iopub.status.idle":"2025-11-27T01:29:26.574653Z","shell.execute_reply.started":"2025-11-27T01:29:26.569323Z","shell.execute_reply":"2025-11-27T01:29:26.573915Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Global variables\nmodels = {}\ntokenizers = {}\npipelines = {}\nllm = None\n\ndef load_qwen_model():\n    \"\"\"Load Qwen model for explanation generation\"\"\"\n    global llm\n    try:\n        print(\"Downloading Qwen model...\")\n        repo_id = \"Qwen/Qwen2.5-1.5B-Instruct-GGUF\" # Using a smaller, faster model for demo\n        model_file = \"qwen2.5-1.5b-instruct-q4_k_m.gguf\"\n        \n        model_dir = snapshot_download(repo_id)\n        \n        print(\"Loading Qwen GGUF model...\")\n        llm = Llama(\n            model_path=f\"{model_dir}/{model_file}\",\n            n_ctx=2048,\n            n_gpu_layers=-1, # Offload all to GPU\n            verbose=False\n        )\n        print(\"Qwen model loaded successfully!\")\n        return True\n    except Exception as e:\n        print(f\"Error loading Qwen model: {e}\")\n        return False\n\ndef load_classification_models():\n    \"\"\"Load all classification models and tokenizers\"\"\"\n    for name, path in MODEL_CONFIGS.items():\n        try:\n            print(f\"Loading {name} from {path}...\")\n            tokenizers[name] = AutoTokenizer.from_pretrained(path)\n            \n            # FIX: Ensure output_attentions is True in config\n            config = AutoConfig.from_pretrained(path)\n            config.output_attentions = True\n            config.output_hidden_states = True\n            config.num_labels = 2 \n            \n            # Load model with config\n            models[name] = AutoModelForSequenceClassification.from_pretrained(\n                path, \n                config=config\n            ).to(device)\n            models[name].eval()\n            \n            # Create pipeline\n            pipelines[name] = pipeline(\n                'text-classification', \n                model=models[name], \n                tokenizer=tokenizers[name], \n                device=device,\n                top_k=None \n            )\n            print(f\"✓ {name} loaded successfully\")\n        except Exception as e:\n            print(f\"✗ Error loading {name}: {e}\")\n\ndef initialize_models():\n    print(\"=\"*50)\n    print(\"Initializing Models...\")\n    load_classification_models()\n    qwen_loaded = load_qwen_model()\n    print(\"=\"*50)\n\n# --- Visualization Functions ---\n\ndef get_attention_visualization(text, model_name):\n    \"\"\"\n    Calculates attention statistics per word and generates visualizations.\n    Returns: DataFrame, Figure (Total Attention), Figure (Average Attention)\n    \"\"\"\n    try:\n        model = models[model_name]\n        tokenizer = tokenizers[model_name]\n        \n        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n        \n        with torch.no_grad():\n            outputs = model(**inputs, output_attentions=True)\n        \n        # Get attention from last layer: [Batch, Heads, Seq_Len, Seq_Len]\n        # Average over heads -> [Seq_Len, Seq_Len]\n        attention_matrix = outputs.attentions[-1][0].mean(dim=0)\n        \n        # Calculate 'Attention Received' by summing columns (how much others attended to this token)\n        # Alternatively, summing rows gives 'Attention Paid' by this token.\n        # 'Received' is often more indicative of importance in classification.\n        attention_scores = attention_matrix.sum(dim=0).cpu().numpy()\n        \n        tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n        \n        # Collect raw data\n        data = []\n        special_tokens = tokenizer.all_special_tokens\n        \n        for token, score in zip(tokens, attention_scores):\n            # Clean up token text (handle BERT ## and RoBERTa Ġ)\n            word = token.replace(\"##\", \"\").replace(\"Ġ\", \"\").strip()\n            \n            # Skip special tokens and empty strings\n            if token in special_tokens or not word:\n                continue\n                \n            data.append({\"Word\": word, \"Score\": score})\n            \n        df_raw = pd.DataFrame(data)\n        \n        if df_raw.empty:\n            return pd.DataFrame(), None, None\n\n        # Aggregate by Word\n        agg_df = df_raw.groupby(\"Word\").agg(\n            Total_Attention=(\"Score\", \"sum\"),\n            Frequency=(\"Score\", \"count\"),\n            Average_Attention=(\"Score\", \"mean\")\n        ).reset_index()\n        \n        # Sort for display\n        agg_df = agg_df.sort_values(by=\"Total_Attention\", ascending=False)\n        \n        # Plot 1: Total Attention\n        fig_total, ax1 = plt.subplots(figsize=(10, 6))\n        top_total = agg_df.head(15)\n        sns.barplot(data=top_total, x=\"Total_Attention\", y=\"Word\", ax=ax1, palette=\"viridis\")\n        ax1.set_title(f\"Total Attention Mass by Word ({model_name})\")\n        plt.tight_layout()\n        \n        # Plot 2: Average Attention\n        fig_avg, ax2 = plt.subplots(figsize=(10, 6))\n        top_avg = agg_df.sort_values(by=\"Average_Attention\", ascending=False).head(15)\n        sns.barplot(data=top_avg, x=\"Average_Attention\", y=\"Word\", ax=ax2, palette=\"magma\")\n        ax2.set_title(f\"Average Attention per Occurrence ({model_name})\")\n        plt.tight_layout()\n\n        return agg_df, fig_total, fig_avg\n\n    except Exception as e:\n        print(f\"Attn Error: {e}\")\n        return pd.DataFrame(), None, None\n\ndef get_integrated_gradients(text, model_name):\n    \"\"\"Generates a bar chart of word attributions.\"\"\"\n    try:\n        model = models[model_name]\n        tokenizer = tokenizers[model_name]\n        \n        explainer = SequenceClassificationExplainer(model, tokenizer)\n        word_attributions = explainer(text) # Returns list of (word, score)\n        \n        # Filter special tokens for cleaner plot\n        filtered = [x for x in word_attributions if x[0] not in ['[CLS]', '[SEP]', '[PAD]', '<s>', '</s>']]\n        # Sort by absolute impact\n        filtered.sort(key=lambda x: abs(x[1]), reverse=True)\n        top_features = filtered[:10] # Top 10\n        \n        words = [x[0] for x in top_features]\n        scores = [x[1] for x in top_features]\n        colors = ['green' if s > 0 else 'red' for s in scores]\n        \n        fig, ax = plt.subplots(figsize=(10, 6))\n        ax.barh(words, scores, color=colors)\n        ax.set_xlabel(\"Attribution Score\")\n        ax.set_title(f\"Top Words Impacting Prediction ({model_name})\")\n        return fig, top_features\n    except Exception as e:\n        print(f\"IG Error: {e}\")\n        return None, []\n\ndef get_shap_values(text, model_name):\n    \"\"\"Generates a bar chart using SHAP values.\"\"\"\n    try:\n        pipe = pipelines[model_name]\n        # Use a generic explainer or partition explainer for text\n        explainer = shap.Explainer(pipe)\n        shap_values = explainer([text])\n        \n        # Extract values for the first sample\n        values = shap_values[0].values\n        if len(values.shape) > 1: # If multi-class, take the predicted class or positive class\n            values = values[:, 1] # Assuming index 1 is 'Depressed'\n            \n        data = values\n        feature_names = shap_values[0].data\n        \n        # Create DataFrame for easy plotting\n        df = pd.DataFrame({'word': feature_names, 'shap': data})\n        df['abs_shap'] = df['shap'].abs()\n        df = df.sort_values('abs_shap', ascending=False).head(10)\n        \n        fig, ax = plt.subplots(figsize=(10, 6))\n        colors = ['blue' if x > 0 else 'orange' for x in df['shap']]\n        ax.barh(df['word'], df['shap'], color=colors)\n        ax.set_title(f\"SHAP Feature Importance ({model_name})\")\n        return fig\n    except Exception as e:\n        print(f\"SHAP Error: {e}\")\n        return None\n\ndef generate_explanation_prompt(text, results, top_tokens):\n    \"\"\"Prepare prompt for Qwen\"\"\"\n    \n    # Format predictions\n    preds_str = \"\"\n    for model, res in results.items():\n        if 'error' not in res:\n            preds_str += f\"- {model}: Predicted {res['predicted_class']} (Confidence: {max(res['probabilities'].values()):.2%})\\n\"\n            \n    # Format key words\n    tokens_str = \", \".join([f\"{t[0]} ({t[1]:.2f})\" for t in top_tokens[:5]])\n    \n    prompt = f\"\"\"<|im_start|>system\nYou are an empathetic AI psychological assistant. Analyze the following classification results.\n<|im_end|>\n<|im_start|>user\nInput Text: \"{text}\"\n\nModel Predictions:\n{preds_str}\n\nKey Influential Words (Integrated Gradients): {tokens_str}\n\nPlease provide:\n1. A summary of the mental state suggested by the text.\n2. Why the models likely predicted this (based on the key words).\n3. A friendly disclaimer that this is an AI tool and not a doctor.\n<|im_end|>\n<|im_start|>assistant\n\"\"\"\n    return prompt\n\ndef call_qwen_model(prompt):\n    if llm is None: \n        return \"Error: Qwen model not loaded.\"\n    \n    output = llm(\n        prompt, \n        max_tokens=512, \n        stop=[\"<|im_end|>\"], \n        echo=False\n    )\n    return output['choices'][0]['text']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T01:29:26.575916Z","iopub.execute_input":"2025-11-27T01:29:26.576129Z","iopub.status.idle":"2025-11-27T01:29:26.629486Z","shell.execute_reply.started":"2025-11-27T01:29:26.576113Z","shell.execute_reply":"2025-11-27T01:29:26.628913Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"initialize_models()\n\nwith gr.Blocks(title=\"Mental Health Analysis\", theme=gr.themes.Soft()) as demo:\n    gr.Markdown(\"# Mental Health Text Analysis & Interpretation\")\n    \n    # State storage\n    stored_text = gr.State()\n    stored_results = gr.State()\n    \n    with gr.Row():\n        with gr.Column(scale=1):\n            text_input = gr.Textbox(label=\"Input Text\", lines=5, placeholder=\"I've been feeling...\")\n            model_selector = gr.CheckboxGroup(choices=list(MODEL_CONFIGS.keys()), value=list(MODEL_CONFIGS.keys())[:1], label=\"Select Models\")\n            analyze_btn = gr.Button(\"Step 1: Analyze\", variant=\"primary\")\n        \n        with gr.Column(scale=1):\n            results_output = gr.Markdown(label=\"Predictions\")\n    \n    # Visualization Section\n    interpret_btn = gr.Button(\"Step 2: Generate Visualizations\", variant=\"secondary\", visible=False)\n    \n    with gr.Tabs(visible=False) as viz_tabs:\n        # Updated Attention Tab with Sub-tabs\n        with gr.Tab(\"Attention Analysis\"):\n            with gr.Tabs():\n                with gr.Tab(\"Total Attention View\"):\n                    attn_plot_total = gr.Plot(label=\"Total Attention by Word\")\n                with gr.Tab(\"Average Attention View\"):\n                    attn_plot_avg = gr.Plot(label=\"Average Attention by Word\")\n                with gr.Tab(\"Data Table\"):\n                    attn_table = gr.Dataframe(label=\"Attention Stats\")\n        \n        with gr.Tab(\"Integrated Gradients\"):\n            ig_plot = gr.Plot()\n        with gr.Tab(\"SHAP Values\"):\n            shap_plot = gr.Plot()\n            \n    # Explanation Section\n    explain_btn = gr.Button(\"Step 3: AI Explanation\", variant=\"secondary\", visible=False)\n    explanation_output = gr.Markdown(visible=False)\n\n    # --- Callbacks ---\n\n    def step1_analyze(text, selected_models):\n        if not text or not selected_models:\n            return \"Please enter text and select a model.\", gr.update(visible=False), text, None\n        \n        results = {}\n        output_md = \"### Model Predictions\\n\"\n        \n        for name in selected_models:\n            if name not in pipelines: continue\n            \n            try:\n                res = pipelines[name](text, return_all_scores=True)\n                probs = {item['label']: item['score'] for item in res[0]}\n                pred_label = max(probs, key=probs.get)\n                \n                results[name] = {\n                    \"predicted_class\": pred_label,\n                    \"probabilities\": probs\n                }\n                \n                output_md += f\"**{name}**: `{pred_label}`\\n\"\n                for l, p in probs.items():\n                    output_md += f\"- {l}: {p:.1%}\\n\"\n                output_md += \"\\n\"\n            except Exception as e:\n                results[name] = {\"error\": str(e)}\n                output_md += f\"**{name}**: Error - {e}\\n\"\n\n        return output_md, gr.update(visible=True), text, results\n\n    def step2_visualize(text, results):\n        if not results: return [None]*6\n        \n        model_name = list(results.keys())[0]\n        \n        # Get Attention Data and Plots\n        attn_df, attn_fig_total, attn_fig_avg = get_attention_visualization(text, model_name)\n        \n        # Get other visualizations\n        ig_fig, top_tokens = get_integrated_gradients(text, model_name)\n        shap_fig = get_shap_values(text, model_name)\n        \n        return (\n            attn_fig_total, \n            attn_fig_avg, \n            attn_df,\n            ig_fig, \n            shap_fig, \n            gr.update(visible=True), # Show tabs\n            gr.update(visible=True)  # Show explain btn\n        )\n\n    def step3_explain_llm(text, results):\n        model_name = list(results.keys())[0]\n        _, top_tokens = get_integrated_gradients(text, model_name)\n        \n        prompt = generate_explanation_prompt(text, results, top_tokens)\n        explanation = call_qwen_model(prompt)\n        return gr.update(value=explanation, visible=True)\n\n    # Wiring\n    analyze_btn.click(\n        step1_analyze, \n        inputs=[text_input, model_selector], \n        outputs=[results_output, interpret_btn, stored_text, stored_results]\n    )\n    \n    interpret_btn.click(\n        step2_visualize,\n        inputs=[stored_text, stored_results],\n        outputs=[attn_plot_total, attn_plot_avg, attn_table, ig_plot, shap_plot, viz_tabs, explain_btn]\n    )\n    \n    explain_btn.click(\n        step3_explain_llm,\n        inputs=[stored_text, stored_results],\n        outputs=[explanation_output]\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T01:29:26.630331Z","iopub.execute_input":"2025-11-27T01:29:26.630629Z","iopub.status.idle":"2025-11-27T01:30:30.048167Z","shell.execute_reply.started":"2025-11-27T01:29:26.630606Z","shell.execute_reply":"2025-11-27T01:30:30.047524Z"}},"outputs":[{"name":"stdout","text":"==================================================\nInitializing Models...\nLoading BERT from /kaggle/input/disorbert-finetuned-models/pytorch/default/1/finetuned-models/bert-base-cased-finetuned/checkpoint-5530...\n","output_type":"stream"},{"name":"stderr","text":"Device set to use cuda\n","output_type":"stream"},{"name":"stdout","text":"✓ BERT loaded successfully\nLoading RoBERTa from /kaggle/input/disorbert-finetuned-models/pytorch/default/1/finetuned-models/roberta-base-finetuned/checkpoint-6320...\n","output_type":"stream"},{"name":"stderr","text":"Device set to use cuda\n","output_type":"stream"},{"name":"stdout","text":"✓ RoBERTa loaded successfully\nLoading MentalBERT from /kaggle/input/disorbert-finetuned-models/pytorch/default/1/finetuned-models/deberta-v3-base-finetuned/checkpoint-4345...\n","output_type":"stream"},{"name":"stderr","text":"Device set to use cuda\n","output_type":"stream"},{"name":"stdout","text":"✓ MentalBERT loaded successfully\nDownloading Qwen model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Fetching 12 files:   0%|          | 0/12 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee3fe00490c44f029f7eff2d85fb5f7c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"qwen2.5-1.5b-instruct-fp16.gguf:   0%|          | 0.00/3.56G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1e511c022994a4ab86e96747b14bbc3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"qwen2.5-1.5b-instruct-q3_k_m.gguf:   0%|          | 0.00/924M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15d961636bc64d65ad1afac7473f44b4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"qwen2.5-1.5b-instruct-q4_0.gguf:   0%|          | 0.00/1.07G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b519e74332b4345ab3cbe28d54f81cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":".gitattributes: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3db9cf77f02e4485854e3314b10f5f8b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"LICENSE: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eef8f12be1854d8ba5d51fabd4261518"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"qwen2.5-1.5b-instruct-q4_k_m.gguf:   0%|          | 0.00/1.12G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"abce6574d3454444ad32e115fa87aa3b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"qwen2.5-1.5b-instruct-q2_k.gguf:   0%|          | 0.00/753M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"215064868e42419b8c7346a2792f82df"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"607b6d7c92524c658b0085f0cc97b876"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"qwen2.5-1.5b-instruct-q5_0.gguf:   0%|          | 0.00/1.26G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"999008bce282439ba01b1afba0065609"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"qwen2.5-1.5b-instruct-q5_k_m.gguf:   0%|          | 0.00/1.29G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd2db4aeb07b4b0d8146ff6d19c41ee7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"qwen2.5-1.5b-instruct-q6_k.gguf:   0%|          | 0.00/1.46G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff50b0e0d9b542248c35f2fe35a97d2c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"qwen2.5-1.5b-instruct-q8_0.gguf:   0%|          | 0.00/1.89G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a8ea084d1224c89b96d5df4352efbd3"}},"metadata":{}},{"name":"stdout","text":"Loading Qwen GGUF model...\n","output_type":"stream"},{"name":"stderr","text":"llama_context: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n","output_type":"stream"},{"name":"stdout","text":"Qwen model loaded successfully!\n==================================================\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"demo.launch(share=True, debug=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T01:30:30.049016Z","iopub.execute_input":"2025-11-27T01:30:30.049319Z","iopub.status.idle":"2025-11-27T01:34:34.887808Z","shell.execute_reply.started":"2025-11-27T01:30:30.049300Z","shell.execute_reply":"2025-11-27T01:34:34.887220Z"}},"outputs":[{"name":"stdout","text":"* Running on local URL:  http://127.0.0.1:7860\n* Running on public URL: https://e465a0a5bdb5ea23c5.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://e465a0a5bdb5ea23c5.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"name":"stderr","text":"`return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\nBertSdpaSelfAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support non-absolute `position_embedding_type` or `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\nRobertaSdpaSelfAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support non-absolute `position_embedding_type` or `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\nYou seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/498 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"PartitionExplainer explainer: 2it [00:13, 13.73s/it]               \n","output_type":"stream"},{"name":"stdout","text":"Keyboard interruption in main thread... closing server.\nKilling tunnel 127.0.0.1:7860 <> https://e465a0a5bdb5ea23c5.gradio.live\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":""},"metadata":{}}],"execution_count":6}]}